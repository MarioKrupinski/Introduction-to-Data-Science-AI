{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️⃣ Lineare Modelle  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| Logistische Regression           | from sklearn.linear_model import LogisticRegression                | Klassischer linearer Klassifikator mit Sigmoid-Funktion.                         | C, solver, penalty                      | Braucht ≥ 100 Samples pro Klasse. Funktioniert nicht gut bei hochgradig korrelierten Features. |\n",
    "| Ridge Classifier                 | from sklearn.linear_model import RidgeClassifier                   | Logistische Regression mit L2-Regularisierung.                                   | alpha, solver                           | Besser als logistische Regression für hochdimensionale Daten (≥ 1.000 Features). |\n",
    "| Lasso Classifier                 | from sklearn.linear_model import Lasso                             | Logistische Regression mit L1-Regularisierung (führt zu sparsamen Modellen).    | alpha                                    | Kann instabil sein bei < 500 Samples.                                                |\n",
    "| Elastic Net Classifier           | from sklearn.linear_model import ElasticNet                        | Kombination aus Ridge- und Lasso-Regularisierung.                               | alpha, l1_ratio                         | Gut bei > 1.000 Features, wenn viele irrelevante Variablen vorhanden sind.          |\n",
    "| Lineare Diskriminanzanalyse (LDA) | from sklearn.discriminant_analysis import LinearDiscriminantAnalysis | Nutzt multivariate Normalverteilungen zur Trennung der Klassen.                  | solver, shrinkage                       | Funktioniert schlecht, wenn Features stark korreliert sind. Mindestens 500+ Samples empfohlen. |\n",
    "| Quadratische Diskriminanzanalyse (QDA) | from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis | Erweitert LDA mit nichtlinearen Entscheidungsgrenzen.                          | reg_param                               | Anfällig für Overfitting bei < 1.000 Samples.                                        |\n",
    "| Perceptron                       | from sklearn.linear_model import Perceptron                        | Einfachstes neuronales Netz, trainiert mit SGD.                                  | penalty, alpha, max_iter                | Nicht für nicht-linear trennbare Daten geeignet.                                    |\n",
    "| SGD Classifier                   | from sklearn.linear_model import SGDClassifier                     | Stochastischer Gradientenabstieg für große Datenmengen.                          | loss, penalty, alpha                    | Empfohlen für > 10.000 Samples.                                                     |\n",
    "| Passive Aggressive Classifier    | from sklearn.linear_model import PassiveAggressiveClassifier       | Online-Learning-Modell für inkrementelles Training.                              | C, max_iter                             | Braucht ≥ 10.000 Datenpunkte für stabiles Training.                                 |\n",
    "\n",
    "2️⃣ Support Vector Machines (SVMs)  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| SVC (Support Vector Classifier)  | from sklearn.svm import SVC                                        | Klassischer SVM für binäre Klassifikation.                                       | C, kernel, gamma                        | Langsam für > 10.000 Samples.                                                       |\n",
    "| LinearSVC                        | from sklearn.svm import LinearSVC                                   | Lineare Version von SVC für große Datenmengen.                                   | C, penalty, loss                        | Besser als SVC für > 50.000 Samples.                                                |\n",
    "| NuSVC                            | from sklearn.svm import NuSVC                                      | Alternative SVM mit Nu-Parameter.                                                 | nu, kernel, gamma                       | Komplexer zu optimieren als SVC.                                                    |\n",
    "\n",
    "3️⃣ Naive Bayes Modelle  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| GaussianNB                       | from sklearn.naive_bayes import GaussianNB                          | Annahme: Normalverteilte Features.                                                | Keine                                   | Braucht ≥ 100 Samples pro Klasse.                                                   |\n",
    "| MultinomialNB                    | from sklearn.naive_bayes import MultinomialNB                       | Für diskrete Merkmale wie Wortfrequenzen.                                        | alpha                                   | Nicht für kontinuierliche Daten geeignet.                                           |\n",
    "| BernoulliNB                      | from sklearn.naive_bayes import BernoulliNB                         | Für binäre Features (z. B. Dokumentklassifikation).                              | alpha                                   | Erwartet binäre oder 0/1-diskretisierte Features.                                   |\n",
    "| ComplementNB                      | from sklearn.naive_bayes import ComplementNB                       | Optimiert für unbalancierte Klassen.                                             | alpha                                   | Besser als MultinomialNB für sehr unbalancierte Daten.                             |\n",
    "\n",
    "4️⃣ Neuronale Netze & Deep Learning  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| MLPClassifier                     | from sklearn.neural_network import MLPClassifier                    | Einfaches neuronales Netz mit Backpropagation.                                   | hidden_layer_sizes, activation          | Benötigt ≥ 5.000 Datenpunkte für zuverlässige Ergebnisse.                           |\n",
    "| Keras Sequential Model            | from tensorflow.keras.models import Sequential                      | Mehrschichtiges neuronales Netz.                                                 | epochs, batch_size, optimizer           | Benötigt GPU für schnelles Training.                                                |\n",
    "| FastAI Tabular Model              | from fastai.tabular.all import tabular_learner                     | Speziell für strukturierte Daten optimiertes Netz.                               | layers, emb_drop, ps                    | Sehr gut für kleine bis mittelgroße Tabellendaten (ab 10.000 Samples).               |\n",
    "| Transformer-Based Classifier      | from transformers import AutoModelForSequenceClassification        | Neuronales Netz auf Basis von BERT/GPT.                                          | learning_rate, num_labels, hidden_size  | Braucht große Datenmengen (50.000+ Samples) und viel Rechenleistung.               |\n",
    "| Capsule Networks                  | from keras.layers import Capsule                                    | Alternative zu CNNs mit besserer Generalisierung.                                 | num_capsule, dim_capsule                | Rechenintensiv, ab 100.000+ Samples empfohlen.                                      |\n",
    "\n",
    "5️⃣ Dichtebasierte Modelle  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| K-Nearest Neighbors (KNN)        | from sklearn.neighbors import KNeighborsClassifier                | Klassifikator, der die Klasse basierend auf den K nächsten Nachbarn vorhersagt.   | n_neighbors, weights, metric            | Empfohlen für kleinere Datensätze (bis ca. 10.000 Samples). Sehr langsam bei großen Datenmengen. |\n",
    "| DBSCAN (Density-Based Spatial Clustering of Applications with Noise) | from sklearn.cluster import DBSCAN  | Dichtebasiertes Clustering, das auch Rauschen und Ausreißer identifizieren kann.  | eps, min_samples, metric                | Nicht gut bei hochdimensionalen Daten (> 10.000 Features). Schwierig bei sehr ungleichmäßigen Dichteverteilungen. |\n",
    "| Nearest Centroid Classifier      | from sklearn.neighbors import NearestCentroid                    | Klassifikator, der die Klasse anhand des nächsten Zentrums eines Clusters vorhersagt. | Keine                                   | Gut für strukturierte, gut trennbare Daten. Kann bei großen Datensätzen langsamer sein. |\n",
    "| Locally Weighted Learning        | from sklearn.neighbors import KNeighborsClassifier                | Speziell für lokale Regression und Klassifikation, bei der jedes Sample unterschiedliche Gewichtungen hat. | n_neighbors, weights                   | Benötigt kleinere Datensätze (bis ca. 5.000). Empfindlich gegenüber Rauschen. |\n",
    "\n",
    "6️⃣ Baumbasierte Verfahren  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| Decision Tree Classifier         | from sklearn.tree import DecisionTreeClassifier                    | Klassischer Baumklassifikator.                                                   | max_depth, min_samples_split, criterion  | Neigt zu Overfitting bei kleinen Datensätzen. Benötigt < 10.000 Samples für stabile Ergebnisse. |\n",
    "| ExtraTreesClassifier             | from sklearn.ensemble import ExtraTreesClassifier                  | Randomisierte Entscheidungsbäume, die weniger anfällig für Overfitting sind.     | n_estimators, max_depth, min_samples_split | Gut für große Datensätze (> 10.000 Samples), aber kann auch sehr rechenintensiv werden. |\n",
    "| Random Forest Classifier         | from sklearn.ensemble import RandomForestClassifier                | Kombiniert viele Entscheidungsbäume (Bagging) für robustere Vorhersagen.         | n_estimators, max_depth, min_samples_split | Benötigt größere Datensätze (> 10.000) und kann speicherintensiv sein. |\n",
    "| HHCART (Hierarchical Clustering Classifier) | from hhc import HHCART                                             | Hierarchischer Klassifikator auf Basis von Clustering-Techniken und Entscheidungsbäumen. | max_depth, n_clusters, criterion       | Funktioniert nicht gut bei sehr kleinen oder zu unstrukturierten Datensätzen. |\n",
    "\n",
    "7️⃣ Ensemble Methoden  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| Bagging Classifier               | from sklearn.ensemble import BaggingClassifier                     | Ensemble-Methode, die mehrere Modelle trainiert und deren Vorhersagen kombiniert. | n_estimators, max_samples, max_features | Empfohlen für ≥ 10.000 Samples. Kann bei kleinen Datensätzen zu Overfitting führen. |\n",
    "| AdaBoost Classifier              | from sklearn.ensemble import AdaBoostClassifier                    | Verstärkt schwache Klassifikatoren, indem fehlerhafte Vorhersagen stärker gewichtet werden. | n_estimators, learning_rate           | Kann bei starkem Overfitting auf kleinen Datensätzen empfindlich sein. |\n",
    "| Stacking Classifier              | from sklearn.ensemble import StackingClassifier                    | Kombiniert mehrere Modelle durch ein Meta-Modell.                                | estimators, final_estimator            | Kann bei sehr unterschiedlichen Modellarten zu instabilen Ergebnissen führen. |\n",
    "| Voting Classifier                | from sklearn.ensemble import VotingClassifier                      | Kombiniert Vorhersagen von mehreren Klassifikatoren, um Robustheit zu erhöhen.    | estimators, voting                     | Gut für Modelle mit ähnlicher Leistung, schlecht bei unterschiedlichem Modellverhalten. |\n",
    "\n",
    "8️⃣ Gradient Boosting Modelle  \n",
    "| Modell                           | Python Package (Import)                                             | Beschreibung                                                                      | Hyperparameter                          | Warnhinweise                                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| GradientBoostingClassifier       | from sklearn.ensemble import GradientBoostingClassifier            | Ensemble von Entscheidungsbäumen, die sequenziell trainiert werden, um Fehler zu korrigieren. | n_estimators, learning_rate, max_depth | Sehr empfindlich gegenüber Overfitting bei kleinen Datensätzen. |\n",
    "| HistGradientBoostingClassifier   | from sklearn.ensemble import HistGradientBoostingClassifier        | Effizientere Version von Gradient Boosting, besonders bei großen Datensätzen.     | n_estimators, max_depth, learning_rate | Benötigt größere Datensätze und ist speicherintensiv. |\n",
    "| XGBClassifier                    | from xgboost import XGBClassifier                                  | Erweiterung von Gradient Boosting mit optimierten Hyperparametern und Regularisierung. | n_estimators, learning_rate, max_depth | Benötigt viel Speicher und Rechenleistung. |\n",
    "| LightGBMClassifier               | from lightgbm import LGBMClassifier                                | Sehr schnelles und effizientes Gradient Boosting-Verfahren, besonders für große Datensätze. | n_estimators, learning_rate, max_depth | Kann bei kleinen Datensätzen overfittinganfällig sein. |\n",
    "| CatBoostClassifier               | from catboost import CatBoostClassifier                            | Gradient Boosting mit spezieller Optimierung für kategoriale Features.           | iterations, learning_rate, depth       | Benötigt gutes Hyperparameter-Tuning für optimale Leistung. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
